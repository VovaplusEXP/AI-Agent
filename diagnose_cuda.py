#!/usr/bin/env python3
"""
Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ CUDA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸ llama-cpp-python

Ğ­Ñ‚Ğ¾Ñ‚ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ:
- Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ»Ğ¸ CUDA Ğ¸ ĞºĞ°ĞºĞ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ
- ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¸ llama-cpp-python CUDA
- Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ»Ğ¸ GPU Ğ´Ğ»Ñ llama-cpp-python
- ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
"""

import sys
import os
from pathlib import Path

def check_cuda_runtime_libraries():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ CUDA runtime Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº"""
    print("=" * 70)
    print("1. ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ CUDA RUNTIME Ğ‘Ğ˜Ğ‘Ğ›Ğ˜ĞĞ¢Ğ•Ğš")
    print("=" * 70)
    
    import subprocess
    import glob
    
    # Ğ˜Ñ‰ĞµĞ¼ libcudart.so Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ
    possible_paths = [
        '/usr/local/cuda/lib64',
        '/usr/local/cuda-12/lib64',
        '/usr/local/cuda-12.*/lib64',
        '/usr/lib/x86_64-linux-gnu',
        '/usr/lib64',
    ]
    
    found_cudart = []
    for pattern in possible_paths:
        matches = glob.glob(pattern)
        for path in matches:
            if os.path.isdir(path):
                cudart_files = glob.glob(os.path.join(path, 'libcudart.so*'))
                if cudart_files:
                    found_cudart.extend(cudart_files)
    
    if found_cudart:
        print(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ñ‹ CUDA runtime Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸:")
        for lib in found_cudart[:3]:  # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 3
            print(f"   {lib}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ»Ğ¸ Ğ¾Ğ½Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ldconfig
        try:
            result = subprocess.run(['ldconfig', '-p'], 
                                  capture_output=True, text=True, timeout=5)
            if 'libcudart.so' in result.stdout:
                print("âœ… libcudart.so Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· ldconfig")
            else:
                print("âš ï¸  libcudart.so ĞĞ• Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· ldconfig")
                print("   ĞÑƒĞ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿ÑƒÑ‚ÑŒ Ğº CUDA Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°Ğ¼ Ğ² LD_LIBRARY_PATH")
        except Exception as e:
            print(f"âš ï¸  ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ldconfig: {e}")
    else:
        print("âŒ CUDA runtime Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ ĞĞ• Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
        print("   Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ CUDA Toolkit")
    
    print()
    return len(found_cudart) > 0

def check_cuda_environment():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ CUDA"""
    print("=" * 70)
    print("2. ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞŸĞ•Ğ Ğ•ĞœĞ•ĞĞĞ«Ğ¥ ĞĞšĞ Ğ£Ğ–Ğ•ĞĞ˜Ğ¯ CUDA")
    print("=" * 70)
    
    cuda_home = os.environ.get('CUDA_HOME') or os.environ.get('CUDA_PATH')
    if cuda_home:
        print(f"âœ… CUDA_HOME: {cuda_home}")
    else:
        print("âš ï¸  CUDA_HOME Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°")
        print("   Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: export CUDA_HOME=/usr/local/cuda")
    
    ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')
    if 'cuda' in ld_library_path.lower():
        print(f"âœ… LD_LIBRARY_PATH ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ CUDA: {ld_library_path}")
    else:
        print("âŒ LD_LIBRARY_PATH Ğ½Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ CUDA")
        print("   Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH")
        print("   Ğ˜Ğ»Ğ¸ Ğ´Ğ»Ñ CUDA 12.x: export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:$LD_LIBRARY_PATH")
    
    print()

def check_cuda_toolkit():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ CUDA Toolkit"""
    print("=" * 70)
    print("3. ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ CUDA TOOLKIT")
    print("=" * 70)
    
    import subprocess
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° nvcc
    try:
        result = subprocess.run(['nvcc', '--version'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            version_line = [l for l in result.stdout.split('\n') if 'release' in l.lower()]
            if version_line:
                print(f"âœ… NVCC ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½: {version_line[0].strip()}")
        else:
            print("âŒ NVCC Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
    except FileNotFoundError:
        print("âŒ NVCC Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² PATH")
        print("   Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ Ñ‡Ñ‚Ğ¾ CUDA Toolkit ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ¸ $CUDA_HOME/bin Ğ² PATH")
    except Exception as e:
        print(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ nvcc: {e}")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total',
                               '--format=csv,noheader'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print(f"âœ… NVIDIA Driver ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
            for line in result.stdout.strip().split('\n'):
                print(f"   GPU: {line}")
        else:
            print("âŒ nvidia-smi Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚")
    except FileNotFoundError:
        print("âŒ nvidia-smi Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        print("   Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ NVIDIA Ğ´Ñ€Ğ°Ğ¹Ğ²ĞµÑ€")
    except Exception as e:
        print(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ nvidia-smi: {e}")
    
    print()

def check_pytorch_cuda():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° CUDA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ² PyTorch"""
    print("=" * 70)
    print("4. ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ PYTORCH CUDA")
    print("=" * 70)
    
    try:
        import torch
        print(f"âœ… PyTorch ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½: {torch.__version__}")
        
        if torch.cuda.is_available():
            print(f"âœ… CUDA Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² PyTorch: {torch.version.cuda}")
            print(f"âœ… Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² CUDA: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("âŒ CUDA ĞĞ• Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² PyTorch")
            print("   ĞŸĞµÑ€ĞµÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ PyTorch Ñ CUDA:")
            print("   pip install torch --index-url https://download.pytorch.org/whl/cu121")
    except ImportError:
        print("âš ï¸  PyTorch Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
    except Exception as e:
        print(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ PyTorch: {e}")
    
    print()

def check_llama_cpp_cuda():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° CUDA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ² llama-cpp-python"""
    print("=" * 70)
    print("5. ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ LLAMA-CPP-PYTHON CUDA")
    print("=" * 70)
    
    try:
        import llama_cpp
        print(f"âœ… llama-cpp-python ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½: {llama_cpp.__version__}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ»Ğ¸ Ñ CUDA
        # Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ n_gpu_layers
        print("\nğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° CUDA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸...")
        print("   (Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ n_gpu_layers=1)")
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ GGUF Ñ„Ğ°Ğ¹Ğ» Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
        # Ğ˜Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· verbose Ğ²Ñ‹Ğ²Ğ¾Ğ´
        import io
        import contextlib
        
        # Ğ—Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        f = io.StringIO()
        try:
            # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ° ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² CUDA
            from llama_cpp import llama_cpp
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ CUDA Ğ±ÑĞºĞµĞ½Ğ´Ğ°
            backends = []
            try:
                # Ğ’ llama.cpp 0.2.0+ ĞµÑÑ‚ÑŒ llama_backend_init
                if hasattr(llama_cpp, 'llama_backend_init'):
                    print("âœ… llama_backend_init Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½")
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ CUDA Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
                cuda_symbols = [
                    'GGML_CUDA',
                    'llama_cublas',
                    'ggml_cuda',
                ]
                
                found_cuda = False
                for symbol in cuda_symbols:
                    if hasattr(llama_cpp, symbol):
                        found_cuda = True
                        print(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½ CUDA ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»: {symbol}")
                        break
                
                if not found_cuda:
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ²ĞµÑ€ÑĞ¸Ğ¸
                    import llama_cpp
                    print("\nâš ï¸  CUDA ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ² llama_cpp")
                    print("   llama-cpp-python ÑĞºĞ¾Ñ€ĞµĞµ Ğ²ÑĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½ Ğ‘Ğ•Ğ— CUDA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸")
                    print("\nğŸ“Œ Ğ Ğ•Ğ¨Ğ•ĞĞ˜Ğ•:")
                    print("   ĞŸĞµÑ€ĞµÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ llama-cpp-python Ñ CUDA:")
                    print()
                    print("   Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1 (Ğ¿Ñ€ĞµĞ´ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ wheels Ğ´Ğ»Ñ CUDA 12.1):")
                    print("   pip uninstall llama-cpp-python -y")
                    print("   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121")
                    print()
                    print("   Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 2 (ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¸ĞºĞ¾Ğ², Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ):")
                    print("   pip uninstall llama-cpp-python -y")
                    print("   CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python --force-reinstall --no-cache-dir")
                    return False
                else:
                    print("\nâœ… llama-cpp-python Ğ¡ĞšĞĞœĞŸĞ˜Ğ›Ğ˜Ğ ĞĞ’ĞĞ Ğ¡ CUDA ĞŸĞĞ”Ğ”Ğ•Ğ Ğ–ĞšĞĞ™")
                    return True
                    
            except Exception as e:
                print(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ CUDA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸: {e}")
                return False
                
        except ImportError as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ° llama_cpp: {e}")
            return False
            
    except ImportError:
        print("âŒ llama-cpp-python Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        print("   Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install llama-cpp-python")
        return False
    except Exception as e:
        print(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ llama-cpp-python: {e}")
        return False
    
    print()

def print_recommendations(has_cudart_libs=False):
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹"""
    print("=" * 70)
    print("6. Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜")
    print("=" * 70)
    
    print("""
ğŸ“Œ ĞŸĞĞ¨ĞĞ“ĞĞ’ĞĞ• Ğ Ğ£ĞšĞĞ’ĞĞ”Ğ¡Ğ¢Ğ’Ğ ĞŸĞ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ®:

1. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ Ñ‡Ñ‚Ğ¾ NVIDIA Ğ´Ñ€Ğ°Ğ¹Ğ²ĞµÑ€ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½:
   nvidia-smi

2. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ Ñ‡Ñ‚Ğ¾ CUDA Toolkit ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½:
   nvcc --version

3. âš ï¸  Ğ’ĞĞ–ĞĞ: Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ğ² ~/.bashrc Ğ¸Ğ»Ğ¸ ~/.zshrc):
   
   # Ğ”Ğ»Ñ CUDA 12.x
   export CUDA_HOME=/usr/local/cuda-12
   export PATH=$CUDA_HOME/bin:$PATH
   export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
   
   # Ğ˜Ğ»Ğ¸ ĞµÑĞ»Ğ¸ CUDA ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ² /usr/local/cuda
   export CUDA_HOME=/usr/local/cuda
   export PATH=$CUDA_HOME/bin:$PATH
   export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
   
   # ĞŸĞ¾ÑĞ»Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ~/.bashrc:
   source ~/.bashrc
   
   # Ğ˜Ğ»Ğ¸ Ğ´Ğ»Ñ ~/.zshrc:
   source ~/.zshrc

4. ğŸ”¥ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ "libcudart.so.12: cannot open shared object file":
   
   ĞĞ°Ğ¹Ğ´Ğ¸Ñ‚Ğµ Ğ³Ğ´Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ libcudart.so:
   find /usr/local -name "libcudart.so*" 2>/dev/null
   find /usr/lib -name "libcudart.so*" 2>/dev/null
   
   Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ² LD_LIBRARY_PATH:
   export LD_LIBRARY_PATH=/Ğ¿ÑƒÑ‚ÑŒ/Ğº/cuda/lib64:$LD_LIBRARY_PATH
   
   ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€:
   export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH

5. ĞŸĞµÑ€ĞµÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ PyTorch Ñ CUDA (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾):
   pip install torch --index-url https://download.pytorch.org/whl/cu121

6. ĞŸĞµÑ€ĞµÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ llama-cpp-python Ñ CUDA:
   
   Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ A (Ğ¿Ñ€ĞµĞ´ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ wheels, Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ):
   pip uninstall llama-cpp-python -y
   
   # Ğ”Ğ»Ñ CUDA 12.4 (ĞºĞ°Ğº Ñƒ Ğ²Ğ°Ñ):
   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124
   
   # Ğ”Ğ»Ñ CUDA 12.1:
   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
   
   Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ B (ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ, Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ):
   pip uninstall llama-cpp-python -y
   CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir

7. ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ terminal Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ÑĞ½Ğ¾Ğ²Ğ°:
   python3 diagnose_cuda.py

8. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ğ°Ğ³ĞµĞ½Ñ‚:
   python cli.py
""")

def main():
    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘         Ğ”Ğ˜ĞĞ“ĞĞĞ¡Ğ¢Ğ˜ĞšĞ CUDA Ğ”Ğ›Ğ¯ AI-AGENT (llama-cpp-python)            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    
    has_cudart = check_cuda_runtime_libraries()
    check_cuda_environment()
    check_cuda_toolkit()
    check_pytorch_cuda()
    has_cuda = check_llama_cpp_cuda()
    
    if not has_cuda or not has_cudart:
        print_recommendations(has_cudart)
        return 1
    else:
        print("=" * 70)
        print("âœ… Ğ’Ğ¡Ğ ĞĞĞ¡Ğ¢Ğ ĞĞ•ĞĞ ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ!")
        print("=" * 70)
        print()
        print("ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ‚ÑŒÑÑ Ğ² VRAM Ğ¿Ñ€Ğ¸ n_gpu_layers=-1")
        print()
        return 0

if __name__ == "__main__":
    sys.exit(main())
