#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ llama-cpp-python

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–º–æ–∂–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å:
- –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª–∏ CUDA –∏ –∫–∞–∫–∞—è –≤–µ—Ä—Å–∏—è
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ llama-cpp-python CUDA
- –î–æ—Å—Ç—É–ø–Ω–∞ –ª–∏ GPU –¥–ª—è llama-cpp-python
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
"""

import sys
import os
from pathlib import Path

def check_cuda_runtime_libraries():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA runtime –±–∏–±–ª–∏–æ—Ç–µ–∫"""
    print("=" * 70)
    print("1. –ü–†–û–í–ï–†–ö–ê CUDA RUNTIME –ë–ò–ë–õ–ò–û–¢–ï–ö")
    print("=" * 70)
    
    import subprocess
    import glob
    
    # –ò—â–µ–º libcudart.so –≤ —Å–∏—Å—Ç–µ–º–µ
    possible_paths = [
        '/usr/local/cuda/lib64',
        '/usr/local/cuda-12/lib64',
        '/usr/local/cuda-12.*/lib64',
        '/usr/lib/x86_64-linux-gnu',
        '/usr/lib64',
    ]
    
    found_cudart = []
    for pattern in possible_paths:
        matches = glob.glob(pattern)
        for path in matches:
            if os.path.isdir(path):
                cudart_files = glob.glob(os.path.join(path, 'libcudart.so*'))
                if cudart_files:
                    found_cudart.extend(cudart_files)
    
    if found_cudart:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã CUDA runtime –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:")
        for lib in found_cudart[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
            print(f"   {lib}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–Ω—ã –ª–∏ –æ–Ω–∏ —á–µ—Ä–µ–∑ ldconfig
        try:
            result = subprocess.run(['ldconfig', '-p'], 
                                  capture_output=True, text=True, timeout=5)
            if 'libcudart.so' in result.stdout:
                print("‚úÖ libcudart.so –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ ldconfig")
            else:
                print("‚ö†Ô∏è  libcudart.so –ù–ï –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ ldconfig")
                print("   –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—É—Ç—å –∫ CUDA –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º –≤ LD_LIBRARY_PATH")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å ldconfig: {e}")
    else:
        print("‚ùå CUDA runtime –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ù–ï –Ω–∞–π–¥–µ–Ω—ã")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA Toolkit")
    
    print()
    return len(found_cudart) > 0

def check_cuda_environment():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è CUDA"""
    print("=" * 70)
    print("2. –ü–†–û–í–ï–†–ö–ê –ü–ï–†–ï–ú–ï–ù–ù–´–• –û–ö–†–£–ñ–ï–ù–ò–Ø CUDA")
    print("=" * 70)
    
    cuda_home = os.environ.get('CUDA_HOME') or os.environ.get('CUDA_PATH')
    if cuda_home:
        print(f"‚úÖ CUDA_HOME: {cuda_home}")
    else:
        print("‚ö†Ô∏è  CUDA_HOME –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: export CUDA_HOME=/usr/local/cuda")
    
    ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')
    if 'cuda' in ld_library_path.lower():
        print(f"‚úÖ LD_LIBRARY_PATH —Å–æ–¥–µ—Ä–∂–∏—Ç CUDA: {ld_library_path}")
    else:
        print("‚ùå LD_LIBRARY_PATH –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç CUDA")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH")
        print("   –ò–ª–∏ –¥–ª—è CUDA 12.x: export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:$LD_LIBRARY_PATH")
    
    print()

def check_cuda_toolkit():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ CUDA Toolkit"""
    print("=" * 70)
    print("3. –ü–†–û–í–ï–†–ö–ê CUDA TOOLKIT")
    print("=" * 70)
    
    import subprocess
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ nvcc
    try:
        result = subprocess.run(['nvcc', '--version'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            version_line = [l for l in result.stdout.split('\n') if 'release' in l.lower()]
            if version_line:
                print(f"‚úÖ NVCC —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {version_line[0].strip()}")
        else:
            print("‚ùå NVCC –Ω–µ –Ω–∞–π–¥–µ–Ω")
    except FileNotFoundError:
        print("‚ùå NVCC –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ PATH")
        print("   –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ CUDA Toolkit —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ $CUDA_HOME/bin –≤ PATH")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ nvcc: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total',
                               '--format=csv,noheader'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print(f"‚úÖ NVIDIA Driver —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            for line in result.stdout.strip().split('\n'):
                print(f"   GPU: {line}")
        else:
            print("‚ùå nvidia-smi –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    except FileNotFoundError:
        print("‚ùå nvidia-smi –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ NVIDIA –¥—Ä–∞–π–≤–µ—Ä")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ nvidia-smi: {e}")
    
    print()

def check_pytorch_cuda():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ PyTorch"""
    print("=" * 70)
    print("4. –ü–†–û–í–ï–†–ö–ê PYTORCH CUDA")
    print("=" * 70)
    
    try:
        import torch
        print(f"‚úÖ PyTorch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {torch.__version__}")
        
        if torch.cuda.is_available():
            print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PyTorch: {torch.version.cuda}")
            print(f"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤ CUDA: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("‚ùå CUDA –ù–ï –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PyTorch")
            print("   –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å CUDA:")
            print("   pip install torch --index-url https://download.pytorch.org/whl/cu121")
    except ImportError:
        print("‚ö†Ô∏è  PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ PyTorch: {e}")
    
    print()

def check_llama_cpp_cuda():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ llama-cpp-python"""
    print("=" * 70)
    print("5. –ü–†–û–í–ï–†–ö–ê LLAMA-CPP-PYTHON CUDA")
    print("=" * 70)
    
    try:
        import llama_cpp
        print(f"‚úÖ llama-cpp-python —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {llama_cpp.__version__}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω –ª–∏ —Å CUDA
        # –≠—Ç–æ –º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ–ø—ã—Ç–∫–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å n_gpu_layers
        print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏...")
        print("   (–ø–æ–ø—ã—Ç–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å n_gpu_layers=1)")
        
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π GGUF —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏
        # –ò–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–º–ø–æ—Ä—Ç –∏ –≤—ã–≤–æ–¥
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ verbose –≤—ã–≤–æ–¥
        import io
        import contextlib
        
        # –ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º –≤—ã–≤–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        f = io.StringIO()
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ —Å–∏–º–≤–æ–ª–æ–≤ CUDA
            from llama_cpp import llama_cpp
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è CUDA –±—ç–∫–µ–Ω–¥–∞
            backends = []
            try:
                # –í llama.cpp 0.2.0+ –µ—Å—Ç—å llama_backend_init
                if hasattr(llama_cpp, 'llama_backend_init'):
                    print("‚úÖ llama_backend_init –¥–æ—Å—Ç—É–ø–µ–Ω")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ CUDA —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª—ã
                cuda_symbols = [
                    'GGML_CUDA',
                    'llama_cublas',
                    'ggml_cuda',
                ]
                
                found_cuda = False
                for symbol in cuda_symbols:
                    if hasattr(llama_cpp, symbol):
                        found_cuda = True
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω CUDA —Å–∏–º–≤–æ–ª: {symbol}")
                        break
                
                if not found_cuda:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ –≤—ã–≤–æ–¥ –≤–µ—Ä—Å–∏–∏
                    import llama_cpp
                    print("\n‚ö†Ô∏è  CUDA —Å–∏–º–≤–æ–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ llama_cpp")
                    print("   llama-cpp-python —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω –ë–ï–ó CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏")
                    print("\nüìå –†–ï–®–ï–ù–ò–ï:")
                    print("   –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python —Å CUDA:")
                    print()
                    print("   –í–∞—Ä–∏–∞–Ω—Ç 1 (–ø—Ä–µ–¥—Å–æ–±—Ä–∞–Ω–Ω—ã–µ wheels –¥–ª—è CUDA 12.1):")
                    print("   pip uninstall llama-cpp-python -y")
                    print("   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121")
                    print()
                    print("   –í–∞—Ä–∏–∞–Ω—Ç 2 (–∫–æ–º–ø–∏–ª—è—Ü–∏—è –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):")
                    print("   pip uninstall llama-cpp-python -y")
                    print("   CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python --force-reinstall --no-cache-dir")
                    return False
                else:
                    print("\n‚úÖ llama-cpp-python –°–ö–û–ú–ü–ò–õ–ò–†–û–í–ê–ù –° CUDA –ü–û–î–î–ï–†–ñ–ö–û–ô")
                    return True
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {e}")
                return False
                
        except ImportError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ llama_cpp: {e}")
            return False
            
    except ImportError:
        print("‚ùå llama-cpp-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-cpp-python")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ llama-cpp-python: {e}")
        return False
    
    print()

def print_recommendations(has_cudart_libs=False):
    """–í—ã–≤–æ–¥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    print("=" * 70)
    print("6. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("=" * 70)
    
    print("""
üìå –ü–û–®–ê–ì–û–í–û–ï –†–£–ö–û–í–û–î–°–¢–í–û –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:

1. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ NVIDIA –¥—Ä–∞–π–≤–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω:
   nvidia-smi

2. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ CUDA Toolkit —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω:
   nvcc --version

3. ‚ö†Ô∏è  –í–ê–ñ–ù–û: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–¥–æ–±–∞–≤—å—Ç–µ –≤ ~/.bashrc –∏–ª–∏ ~/.zshrc):
   
   # –î–ª—è CUDA 12.x
   export CUDA_HOME=/usr/local/cuda-12
   export PATH=$CUDA_HOME/bin:$PATH
   export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
   
   # –ò–ª–∏ –µ—Å–ª–∏ CUDA —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤ /usr/local/cuda
   export CUDA_HOME=/usr/local/cuda
   export PATH=$CUDA_HOME/bin:$PATH
   export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
   
   # –ü–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è ~/.bashrc:
   source ~/.bashrc
   
   # –ò–ª–∏ –¥–ª—è ~/.zshrc:
   source ~/.zshrc

4. üî• –ö–†–ò–¢–ò–ß–ù–û –ø—Ä–∏ –æ—à–∏–±–∫–µ "libcudart.so.12: cannot open shared object file":
   
   –ù–∞–π–¥–∏—Ç–µ –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è libcudart.so:
   find /usr/local -name "libcudart.so*" 2>/dev/null
   find /usr/lib -name "libcudart.so*" 2>/dev/null
   
   –ó–∞—Ç–µ–º –¥–æ–±–∞–≤—å—Ç–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–π –ø—É—Ç—å –≤ LD_LIBRARY_PATH:
   export LD_LIBRARY_PATH=/–ø—É—Ç—å/–∫/cuda/lib64:$LD_LIBRARY_PATH
   
   –ù–∞–ø—Ä–∏–º–µ—Ä:
   export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH

5. –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å CUDA (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ):
   pip install torch --index-url https://download.pytorch.org/whl/cu121

6. –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python —Å CUDA:
   
   –í–∞—Ä–∏–∞–Ω—Ç A (–ø—Ä–µ–¥—Å–æ–±—Ä–∞–Ω–Ω—ã–µ wheels, –±—ã—Å—Ç—Ä–µ–µ):
   pip uninstall llama-cpp-python -y
   
   # –î–ª—è CUDA 12.4 (–∫–∞–∫ —É –≤–∞—Å):
   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124
   
   # –î–ª—è CUDA 12.1:
   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
   
   –í–∞—Ä–∏–∞–Ω—Ç B (–∫–æ–º–ø–∏–ª—è—Ü–∏—è, –ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å):
   pip uninstall llama-cpp-python -y
   CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir

7. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ terminal –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–Ω–æ–≤–∞:
   python3 diagnose_cuda.py

8. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–≥–µ–Ω—Ç:
   python cli.py
""")

def main():
    print()
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë         –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê CUDA –î–õ–Ø AI-AGENT (llama-cpp-python)            ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print()
    
    has_cudart = check_cuda_runtime_libraries()
    check_cuda_environment()
    check_cuda_toolkit()
    check_pytorch_cuda()
    has_cuda = check_llama_cpp_cuda()
    
    if not has_cuda or not has_cudart:
        print_recommendations(has_cudart)
        return 1
    else:
        print("=" * 70)
        print("‚úÖ –í–°–Å –ù–ê–°–¢–†–û–ï–ù–û –ü–†–ê–í–ò–õ–¨–ù–û!")
        print("=" * 70)
        print()
        print("–ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –≤ VRAM –ø—Ä–∏ n_gpu_layers=-1")
        print()
        return 0

if __name__ == "__main__":
    sys.exit(main())
