#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ llama-cpp-python

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–º–æ–∂–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å:
- –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª–∏ CUDA –∏ –∫–∞–∫–∞—è –≤–µ—Ä—Å–∏—è
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ llama-cpp-python CUDA
- –î–æ—Å—Ç—É–ø–Ω–∞ –ª–∏ GPU –¥–ª—è llama-cpp-python
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
"""

import sys
import os
from pathlib import Path

def check_cuda_environment():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è CUDA"""
    print("=" * 70)
    print("1. –ü–†–û–í–ï–†–ö–ê –ü–ï–†–ï–ú–ï–ù–ù–´–• –û–ö–†–£–ñ–ï–ù–ò–Ø CUDA")
    print("=" * 70)
    
    cuda_home = os.environ.get('CUDA_HOME') or os.environ.get('CUDA_PATH')
    if cuda_home:
        print(f"‚úÖ CUDA_HOME: {cuda_home}")
    else:
        print("‚ö†Ô∏è  CUDA_HOME –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: export CUDA_HOME=/usr/local/cuda")
    
    ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')
    if 'cuda' in ld_library_path.lower():
        print(f"‚úÖ LD_LIBRARY_PATH —Å–æ–¥–µ—Ä–∂–∏—Ç CUDA")
    else:
        print("‚ö†Ô∏è  LD_LIBRARY_PATH –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç CUDA")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH")
    
    print()

def check_cuda_toolkit():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ CUDA Toolkit"""
    print("=" * 70)
    print("2. –ü–†–û–í–ï–†–ö–ê CUDA TOOLKIT")
    print("=" * 70)
    
    import subprocess
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ nvcc
    try:
        result = subprocess.run(['nvcc', '--version'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            version_line = [l for l in result.stdout.split('\n') if 'release' in l.lower()]
            if version_line:
                print(f"‚úÖ NVCC —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {version_line[0].strip()}")
        else:
            print("‚ùå NVCC –Ω–µ –Ω–∞–π–¥–µ–Ω")
    except FileNotFoundError:
        print("‚ùå NVCC –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ PATH")
        print("   –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ CUDA Toolkit —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ $CUDA_HOME/bin –≤ PATH")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ nvcc: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total',
                               '--format=csv,noheader'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print(f"‚úÖ NVIDIA Driver —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            for line in result.stdout.strip().split('\n'):
                print(f"   GPU: {line}")
        else:
            print("‚ùå nvidia-smi –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    except FileNotFoundError:
        print("‚ùå nvidia-smi –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ NVIDIA –¥—Ä–∞–π–≤–µ—Ä")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ nvidia-smi: {e}")
    
    print()

def check_pytorch_cuda():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ PyTorch"""
    print("=" * 70)
    print("3. –ü–†–û–í–ï–†–ö–ê PYTORCH CUDA")
    print("=" * 70)
    
    try:
        import torch
        print(f"‚úÖ PyTorch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {torch.__version__}")
        
        if torch.cuda.is_available():
            print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PyTorch: {torch.version.cuda}")
            print(f"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤ CUDA: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("‚ùå CUDA –ù–ï –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PyTorch")
            print("   –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å CUDA:")
            print("   pip install torch --index-url https://download.pytorch.org/whl/cu121")
    except ImportError:
        print("‚ö†Ô∏è  PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ PyTorch: {e}")
    
    print()

def check_llama_cpp_cuda():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ llama-cpp-python"""
    print("=" * 70)
    print("4. –ü–†–û–í–ï–†–ö–ê LLAMA-CPP-PYTHON CUDA")
    print("=" * 70)
    
    try:
        import llama_cpp
        print(f"‚úÖ llama-cpp-python —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {llama_cpp.__version__}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω –ª–∏ —Å CUDA
        # –≠—Ç–æ –º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ–ø—ã—Ç–∫–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å n_gpu_layers
        print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏...")
        print("   (–ø–æ–ø—ã—Ç–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å n_gpu_layers=1)")
        
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π GGUF —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏
        # –ò–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–º–ø–æ—Ä—Ç –∏ –≤—ã–≤–æ–¥
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ verbose –≤—ã–≤–æ–¥
        import io
        import contextlib
        
        # –ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º –≤—ã–≤–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        f = io.StringIO()
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ —Å–∏–º–≤–æ–ª–æ–≤ CUDA
            from llama_cpp import llama_cpp
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è CUDA –±—ç–∫–µ–Ω–¥–∞
            backends = []
            try:
                # –í llama.cpp 0.2.0+ –µ—Å—Ç—å llama_backend_init
                if hasattr(llama_cpp, 'llama_backend_init'):
                    print("‚úÖ llama_backend_init –¥–æ—Å—Ç—É–ø–µ–Ω")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ CUDA —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª—ã
                cuda_symbols = [
                    'GGML_CUDA',
                    'llama_cublas',
                    'ggml_cuda',
                ]
                
                found_cuda = False
                for symbol in cuda_symbols:
                    if hasattr(llama_cpp, symbol):
                        found_cuda = True
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω CUDA —Å–∏–º–≤–æ–ª: {symbol}")
                        break
                
                if not found_cuda:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ –≤—ã–≤–æ–¥ –≤–µ—Ä—Å–∏–∏
                    import llama_cpp
                    print("\n‚ö†Ô∏è  CUDA —Å–∏–º–≤–æ–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ llama_cpp")
                    print("   llama-cpp-python —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω –ë–ï–ó CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏")
                    print("\nüìå –†–ï–®–ï–ù–ò–ï:")
                    print("   –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python —Å CUDA:")
                    print()
                    print("   –í–∞—Ä–∏–∞–Ω—Ç 1 (–ø—Ä–µ–¥—Å–æ–±—Ä–∞–Ω–Ω—ã–µ wheels –¥–ª—è CUDA 12.1):")
                    print("   pip uninstall llama-cpp-python -y")
                    print("   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121")
                    print()
                    print("   –í–∞—Ä–∏–∞–Ω—Ç 2 (–∫–æ–º–ø–∏–ª—è—Ü–∏—è –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):")
                    print("   pip uninstall llama-cpp-python -y")
                    print("   CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python --force-reinstall --no-cache-dir")
                    return False
                else:
                    print("\n‚úÖ llama-cpp-python –°–ö–û–ú–ü–ò–õ–ò–†–û–í–ê–ù –° CUDA –ü–û–î–î–ï–†–ñ–ö–û–ô")
                    return True
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {e}")
                return False
                
        except ImportError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ llama_cpp: {e}")
            return False
            
    except ImportError:
        print("‚ùå llama-cpp-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-cpp-python")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ llama-cpp-python: {e}")
        return False
    
    print()

def print_recommendations():
    """–í—ã–≤–æ–¥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    print("=" * 70)
    print("5. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("=" * 70)
    
    print("""
üìå –ü–û–®–ê–ì–û–í–û–ï –†–£–ö–û–í–û–î–°–¢–í–û –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:

1. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ NVIDIA –¥—Ä–∞–π–≤–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω:
   nvidia-smi

2. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ CUDA Toolkit —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω:
   nvcc --version

3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–¥–æ–±–∞–≤—å—Ç–µ –≤ ~/.bashrc):
   export CUDA_HOME=/usr/local/cuda
   export PATH=$CUDA_HOME/bin:$PATH
   export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

4. –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å CUDA (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ):
   pip install torch --index-url https://download.pytorch.org/whl/cu121

5. –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python —Å CUDA:
   
   –í–∞—Ä–∏–∞–Ω—Ç A (–ø—Ä–µ–¥—Å–æ–±—Ä–∞–Ω–Ω—ã–µ wheels, –±—ã—Å—Ç—Ä–µ–µ):
   pip uninstall llama-cpp-python -y
   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
   
   –í–∞—Ä–∏–∞–Ω—Ç B (–∫–æ–º–ø–∏–ª—è—Ü–∏—è, –ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å):
   pip uninstall llama-cpp-python -y
   CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir

6. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ terminal –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–Ω–æ–≤–∞:
   python3 diagnose_cuda.py

7. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–≥–µ–Ω—Ç:
   python cli.py
""")

def main():
    print()
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë         –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê CUDA –î–õ–Ø AI-AGENT (llama-cpp-python)            ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print()
    
    check_cuda_environment()
    check_cuda_toolkit()
    check_pytorch_cuda()
    has_cuda = check_llama_cpp_cuda()
    
    if not has_cuda:
        print_recommendations()
        return 1
    else:
        print("=" * 70)
        print("‚úÖ –í–°–Å –ù–ê–°–¢–†–û–ï–ù–û –ü–†–ê–í–ò–õ–¨–ù–û!")
        print("=" * 70)
        print()
        print("–ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –≤ VRAM –ø—Ä–∏ n_gpu_layers=-1")
        print()
        return 0

if __name__ == "__main__":
    sys.exit(main())
