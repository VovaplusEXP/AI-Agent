"""
–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ Context Manager v2.2.0
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –±–∞–∑–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from context_manager import ContextManager
from memory import MemoryManager
from pathlib import Path
import tempfile
import shutil


def test_adaptive_context_basic():
    """
    –ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–Ω–µ–¥–∂–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç
    –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    """
    temp_dir = tempfile.mkdtemp()
    
    try:
        # Mock LLM
        class MockLLM:
            def tokenize(self, text):
                return text.encode('utf-8')[:len(text)//4]
        
        # –°–æ–∑–¥–∞—ë–º memory manager
        memory = MemoryManager(
            global_memory_dir=str(Path(temp_dir) / "global"),
            chats_base_dir=str(Path(temp_dir) / "chats")
        )
        project_memory = memory.get_project_memory("test")
        
        # –°–æ–∑–¥–∞—ë–º context manager
        cm = ContextManager(
            llm=MockLLM(),
            global_memory=memory.global_memory,
            project_memory=project_memory,
            max_tokens=20480
        )
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        system_prompt = "–¢—ã AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç" * 100
        scratchpad = {"goal": "Test", "plan": "Step 1\nStep 2", "last_action_result": "ok"}
        history = [
            {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç"},
            {"role": "assistant", "content": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!"}
        ]
        
        # –í—ã–∑—ã–≤–∞–µ–º build_context
        context_messages, stats, enhanced_prompt = cm.build_context(
            system_prompt=system_prompt,
            scratchpad=scratchpad,
            history=history,
            current_query="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å"
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        assert isinstance(context_messages, list), "–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º"
        assert isinstance(stats, dict), "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä—ë–º"
        assert isinstance(enhanced_prompt, str), "–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–π –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ
        assert 'total_tokens' in stats
        assert 'system_tokens' in stats
        assert 'scratchpad_tokens' in stats
        assert 'memory_tokens' in stats
        assert 'history_tokens' in stats
        assert 'budget_redistribution' in stats  # –ù–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç–∏
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç –ª–∏–º–∏—Ç
        assert stats['total_tokens'] <= cm.max_tokens, \
            f"–¢–æ–∫–µ–Ω—ã {stats['total_tokens']} –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç {cm.max_tokens}"
        
        print(f"‚úÖ –ë–ê–ó–û–í–´–ô –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù")
        print(f"   –¢–æ–∫–µ–Ω–æ–≤: {stats['total_tokens']}/{cm.max_tokens}")
        print(f"   –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è: {stats.get('utilization', 0):.1f}%")
        print(f"   –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {stats['budget_redistribution']}")
        
    finally:
        shutil.rmtree(temp_dir)


def test_adaptive_empty_memory():
    """
    –¢–µ—Å—Ç –ø—É—Å—Ç–æ–π –ø–∞–º—è—Ç–∏: –¢–æ–∫–µ–Ω—ã –¥–æ–ª–∂–Ω—ã –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å—Å—è –∏—Å—Ç–æ—Ä–∏–∏
    """
    temp_dir = tempfile.mkdtemp()
    
    try:
        class MockLLM:
            def tokenize(self, text):
                return text.encode('utf-8')[:len(text)//4]
        
        memory = MemoryManager(
            global_memory_dir=str(Path(temp_dir) / "global"),
            chats_base_dir=str(Path(temp_dir) / "chats")
        )
        project_memory = memory.get_project_memory("test")
        
        cm = ContextManager(
            llm=MockLLM(),
            global_memory=memory.global_memory,
            project_memory=project_memory,
            max_tokens=20480
        )
        
        # –ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –∏ scratchpad
        system_prompt = "–¢—ã AI"
        scratchpad = {"goal": "Test", "plan": "A", "last_action_result": "ok"}
        
        # –ë–æ–ª—å—à–∞—è –∏—Å—Ç–æ—Ä–∏—è (30 —Å–æ–æ–±—â–µ–Ω–∏–π)
        history = []
        for i in range(30):
            history.append({
                "role": "user" if i % 2 == 0 else "assistant",
                "content": f"–°–æ–æ–±—â–µ–Ω–∏–µ {i}: –¥–µ—Ç–∞–ª–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç" * 20
            })
        
        context_messages, stats, _ = cm.build_context(
            system_prompt=system_prompt,
            scratchpad=scratchpad,
            history=history,
            current_query="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å"
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        # 1. –ü–∞–º—è—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—É—Å—Ç–∞—è –∏–ª–∏ –±–ª–∏–∑–∫–∞ –∫ –Ω—É–ª—é
        assert stats['memory_tokens'] < 500, \
            f"–ü–∞–º—è—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–∞–ª–æ–π (–ø—É—Å—Ç–∞—è), –ø–æ–ª—É—á–µ–Ω–æ {stats['memory_tokens']}"
        
        # 2. –ò—Å—Ç–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ –ø–æ–ª—É—á–∏—Ç—å –º–Ω–æ–≥–æ –º–µ—Å—Ç–∞
        history_percent = (stats['history_tokens'] / cm.max_tokens) * 100
        assert history_percent > 40, \
            f"–ò—Å—Ç–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ –ø–æ–ª—É—á–∏—Ç—å >40% –ø—Ä–∏ –ø—É—Å—Ç–æ–π –ø–∞–º—è—Ç–∏, –ø–æ–ª—É—á–µ–Ω–æ {history_percent:.1f}%"
        
        # 3. –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (l3_saved > 0)
        redist = stats['budget_redistribution']
        assert 'l3_saved' in redist, "–î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–µ—Ç—Ä–∏–∫–∞ l3_saved"
        
        print(f"‚úÖ –¢–ï–°–¢ –ü–£–°–¢–û–ô –ü–ê–ú–Ø–¢–ò –ü–†–û–ô–î–ï–ù")
        print(f"   –ò—Å—Ç–æ—Ä–∏—è: {history_percent:.1f}% ({stats['history_tokens']} —Ç–æ–∫–µ–Ω–æ–≤)")
        print(f"   –ü–∞–º—è—Ç—å —Å—ç–∫–æ–Ω–æ–º–∏–ª–∞: {redist.get('l3_saved', 0)} —Ç–æ–∫–µ–Ω–æ–≤")
        
    finally:
        shutil.rmtree(temp_dir)


def test_adaptive_large_memory():
    """
    –¢–µ—Å—Ç –±–æ–ª—å—à–æ–π –ø–∞–º—è—Ç–∏: –ü–∞–º—è—Ç—å –¥–æ–ª–∂–Ω–∞ —Ä–∞—Å—à–∏—Ä—è—Ç—å—Å—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ
    """
    temp_dir = tempfile.mkdtemp()
    
    try:
        class MockLLM:
            def tokenize(self, text):
                return text.encode('utf-8')[:len(text)//4]
        
        memory = MemoryManager(
            global_memory_dir=str(Path(temp_dir) / "global"),
            chats_base_dir=str(Path(temp_dir) / "chats")
        )
        project_memory = memory.get_project_memory("test")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ –∑–∞–ø–∏—Å–µ–π
        for i in range(30):
            memory.global_memory.add(
                text=f"–ì–ª–æ–±–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å {i}: –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏" * 15,
                metadata={"index": i}
            )
            project_memory.add(
                text=f"–ü—Ä–æ–µ–∫—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å {i}: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è" * 15,
                metadata={"index": i}
            )
        
        cm = ContextManager(
            llm=MockLLM(),
            global_memory=memory.global_memory,
            project_memory=project_memory,
            max_tokens=20480
        )
        
        system_prompt = "–¢—ã AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
        scratchpad = {"goal": "–ê–Ω–∞–ª–∏–∑", "plan": "–®–∞–≥ 1", "last_action_result": "ok"}
        history = [{"role": "user", "content": "–ù–∞–π–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø–∏—Å—è—Ö"}]
        
        context_messages, stats, _ = cm.build_context(
            system_prompt=system_prompt,
            scratchpad=scratchpad,
            history=history,
            current_query="–∑–∞–ø–∏—Å–∏ –∏ –¥–µ—Ç–∞–ª–∏"  # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        # 1. –ü–∞–º—è—Ç—å –¥–æ–ª–∂–Ω–∞ –∑–∞–Ω–∏–º–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ
        memory_percent = (stats['memory_tokens'] / cm.max_tokens) * 100
        assert memory_percent >= 10, \
            f"–ü–∞–º—è—Ç—å –¥–æ–ª–∂–Ω–∞ –∑–∞–Ω–∏–º–∞—Ç—å ‚â•10% –ø—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∑–∞–ø–∏—Å–µ–π, –ø–æ–ª—É—á–µ–Ω–æ {memory_percent:.1f}%"
        
        # 2. –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        assert 'budget_redistribution' in stats
        
        print(f"‚úÖ –¢–ï–°–¢ –ë–û–õ–¨–®–û–ô –ü–ê–ú–Ø–¢–ò –ü–†–û–ô–î–ï–ù")
        print(f"   –ü–∞–º—è—Ç—å: {memory_percent:.1f}% ({stats['memory_tokens']} —Ç–æ–∫–µ–Ω–æ–≤)")
        print(f"   –ò—Å—Ç–æ—Ä–∏—è: {(stats['history_tokens']/cm.max_tokens)*100:.1f}%")
        
    finally:
        shutil.rmtree(temp_dir)


def test_adaptive_critical_components():
    """
    –¢–µ—Å—Ç –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: –û–≥—Ä–æ–º–Ω—ã–π scratchpad –ù–ï –¥–æ–ª–∂–µ–Ω –æ–±—Ä–µ–∑–∞—Ç—å—Å—è
    """
    temp_dir = tempfile.mkdtemp()
    
    try:
        class MockLLM:
            def tokenize(self, text):
                return text.encode('utf-8')[:len(text)//4]
        
        memory = MemoryManager(
            global_memory_dir=str(Path(temp_dir) / "global"),
            chats_base_dir=str(Path(temp_dir) / "chats")
        )
        project_memory = memory.get_project_memory("test")
        
        cm = ContextManager(
            llm=MockLLM(),
            global_memory=memory.global_memory,
            project_memory=project_memory,
            max_tokens=20480
        )
        
        # –û–≥—Ä–æ–º–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏ scratchpad
        system_prompt = "–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏" * 300
        huge_plan = "\n".join([f"{i}. –®–∞–≥ {i}: –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ" * 5 for i in range(1, 31)])
        scratchpad = {
            "goal": "–°–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ü–µ–ª–µ–π –∏ –æ–∂–∏–¥–∞–Ω–∏–π" * 10,
            "plan": huge_plan,
            "last_action_result": "–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è —Å –¥–µ—Ç–∞–ª—è–º–∏" * 20
        }
        
        history = [{"role": "user", "content": "–¢–µ—Å—Ç"}]
        
        context_messages, stats, enhanced_prompt = cm.build_context(
            system_prompt=system_prompt,
            scratchpad=scratchpad,
            history=history,
            current_query="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å"
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        # 1. Scratchpad –¥–æ–ª–∂–µ–Ω –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1)
        assert scratchpad["goal"][:50] in enhanced_prompt or \
               "–¶–µ–ª—å:" in enhanced_prompt, \
               "–¶–µ–ª—å –∏–∑ scratchpad –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (–∫—Ä–∏—Ç–∏—á–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç)"
        
        # 2. –ö–æ–Ω—Ç–µ–∫—Å—Ç –ù–ï –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç
        assert stats['total_tokens'] <= cm.max_tokens, \
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç: {stats['total_tokens']} > {cm.max_tokens}"
        
        # 3. –ò—Å—Ç–æ—Ä–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —É—Ä–µ–∑–∞–Ω–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3), –Ω–æ –º–∏–Ω–∏–º—É–º –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å
        assert stats['history_tokens'] >= 0, "–ò—Å—Ç–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å"
        
        print(f"‚úÖ –¢–ï–°–¢ –ö–†–ò–¢–ò–ß–ù–´–• –ö–û–ú–ü–û–ù–ï–ù–¢–û–í –ü–†–û–ô–î–ï–ù")
        print(f"   Scratchpad: {stats['scratchpad_tokens']} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"   System: {stats['system_tokens']} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"   –ò—Å—Ç–æ—Ä–∏—è: {stats['history_tokens']} —Ç–æ–∫–µ–Ω–æ–≤")
        
    finally:
        shutil.rmtree(temp_dir)


if __name__ == "__main__":
    print("\n" + "="*70)
    print("üöÄ –¢–ï–°–¢–´ –ê–î–ê–ü–¢–ò–í–ù–û–ì–û CONTEXT MANAGER v2.2.0")
    print("="*70 + "\n")
    
    test_adaptive_context_basic()
    print()
    test_adaptive_empty_memory()
    print()
    test_adaptive_large_memory()
    print()
    test_adaptive_critical_components()
    
    print("\n" + "="*70)
    print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û")
    print("="*70 + "\n")
