# –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ RAM –≤–º–µ—Å—Ç–æ VRAM

**–î–∞—Ç–∞:** 2025-10-18  
**–í–µ—Ä—Å–∏—è:** v0.0.3-p5-alpha  
**–ü—Ä–æ–±–ª–µ–º–∞:** –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ RAM –≤–º–µ—Å—Ç–æ VRAM –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ CUDA 12.x

---

## üî¥ –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –ü—Ä–æ–±–ª–µ–º–∞ 1: llama-cpp-python –±–µ–∑ CUDA

**–°–∏–º–ø—Ç–æ–º—ã:**
- –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ RAM –≤–º–µ—Å—Ç–æ VRAM
- –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ 0 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)
- GPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ

**–ü—Ä–∏—á–∏–Ω–∞:** llama-cpp-python —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ë–ï–ó CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ (CPU-only –≤–µ—Ä—Å–∏—è)

### –ü—Ä–æ–±–ª–µ–º–∞ 2: libcudart.so.12 –Ω–µ –Ω–∞–π–¥–µ–Ω–∞

**–°–∏–º–ø—Ç–æ–º—ã:**
```
OSError: libcudart.so.12: cannot open shared object file: No such file or directory
```

**–ü—Ä–∏—á–∏–Ω–∞:** CUDA runtime –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ –≤ LD_LIBRARY_PATH, –¥–∞–∂–µ –µ—Å–ª–∏ llama-cpp-python —Å CUDA —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ

---

## ‚úÖ –†–µ—à–µ–Ω–∏–µ

### –ë—ã—Å—Ç—Ä–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è libcudart.so.12

–ï—Å–ª–∏ –≤—ã –≤–∏–¥–∏—Ç–µ –æ—à–∏–±–∫—É `libcudart.so.12: cannot open shared object file`, –∑–∞–ø—É—Å—Ç–∏—Ç–µ:

```bash
python3 fix_libcudart.py
```

–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π–¥–µ—Ç CUDA –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –ø–æ–¥—Å–∫–∞–∂–µ—Ç –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.

**–†—É—á–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:**

1. –ù–∞–π–¥–∏—Ç–µ –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è libcudart.so:
```bash
find /usr/local -name "libcudart.so*" 2>/dev/null
find /usr/lib -name "libcudart.so*" 2>/dev/null
```

2. –î–æ–±–∞–≤—å—Ç–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–π –ø—É—Ç—å –≤ LD_LIBRARY_PATH:
```bash
# –î–ª—è CUDA 12.4 (–ø—Ä–∏–º–µ—Ä)
export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH

# –ò–ª–∏ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
```

3. –î–æ–±–∞–≤—å—Ç–µ –≤ ~/.bashrc –∏–ª–∏ ~/.zshrc –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞:
```bash
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

### –ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç:

```bash
python3 diagnose_cuda.py
```

–°–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç:
- ‚úÖ CUDA runtime –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (libcudart.so)
- ‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (LD_LIBRARY_PATH)
- ‚úÖ CUDA Toolkit –∏ NVIDIA –¥—Ä–∞–π–≤–µ—Ä
- ‚úÖ PyTorch CUDA –ø–æ–¥–¥–µ—Ä–∂–∫—É
- ‚úÖ llama-cpp-python CUDA –ø–æ–¥–¥–µ—Ä–∂–∫—É

### –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA

–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ CUDA —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞:

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ NVIDIA –¥—Ä–∞–π–≤–µ—Ä–∞
nvidia-smi

# –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA Toolkit
nvcc --version
```

–ï—Å–ª–∏ –∫–æ–º–∞–Ω–¥—ã –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ NVIDIA –¥—Ä–∞–π–≤–µ—Ä –∏ CUDA Toolkit:
- [NVIDIA Driver Download](https://www.nvidia.com/download/index.aspx)
- [CUDA Toolkit Download](https://developer.nvidia.com/cuda-downloads)

### –®–∞–≥ 3: –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∞ llama-cpp-python —Å CUDA

#### –í–∞—Ä–∏–∞–Ω—Ç A: –ü—Ä–µ–¥—Å–æ–±—Ä–∞–Ω–Ω—ã–µ CUDA wheels (–±—ã—Å—Ç—Ä–µ–µ)

**–î–ª—è CUDA 12.1:**
```bash
pip uninstall llama-cpp-python -y
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

**–î–ª—è CUDA 11.8:**
```bash
pip uninstall llama-cpp-python -y
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu118
```

#### –í–∞—Ä–∏–∞–Ω—Ç B: –ö–æ–º–ø–∏–ª—è—Ü–∏—è –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤ (–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)

```bash
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –î–ª—è –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ –Ω—É–∂–Ω—ã:
- CUDA Toolkit
- CMake: `sudo apt install cmake` –∏–ª–∏ `brew install cmake`
- GCC/Clang –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä

### –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞:

```python
import llama_cpp
print("llama-cpp-python version:", llama_cpp.__version__)

# –ü–æ–ø—ã—Ç–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å GPU
from llama_cpp import Llama
llm = Llama(
    model_path="your_model.gguf",
    n_gpu_layers=1,  # –•–æ—Ç—è –±—ã 1 —Å–ª–æ–π –Ω–∞ GPU
    verbose=True     # –ü–æ–∫–∞–∂–µ—Ç –≥–¥–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å
)
```

–í verbose –≤—ã–≤–æ–¥–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å:
```
llama_model_load: using CUDA ...
llama_kv_cache_init: CUDA ...
```

–ï—Å–ª–∏ –≤–∏–¥–∏—Ç–µ —Ç–æ–ª—å–∫–æ CPU - llama-cpp-python –≤—Å—ë –µ—â—ë –±–µ–∑ CUDA.

### –®–∞–≥ 5: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

–î–æ–±–∞–≤—å—Ç–µ –≤ `~/.bashrc` –∏–ª–∏ `~/.zshrc`:

```bash
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω–∞–ª:
```bash
source ~/.bashrc
```

---

## üîç –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã

### Q: –û—à–∏–±–∫–∞ "libcudart.so.12: cannot open shared object file"
A: –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ llama-cpp-python —Å CUDA —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ü–†–ê–í–ò–õ–¨–ù–û, –Ω–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ CUDA runtime –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.
   
   **–ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ:**
   ```bash
   python3 fix_libcudart.py
   ```
   
   **–†—É—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ:**
   ```bash
   # –ù–∞–π–¥–∏—Ç–µ libcudart.so
   find /usr/local -name "libcudart.so*" 2>/dev/null
   
   # –î–æ–±–∞–≤—å—Ç–µ –ø—É—Ç—å –≤ LD_LIBRARY_PATH
   export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH
   
   # –î–æ–±–∞–≤—å—Ç–µ –≤ ~/.bashrc –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞
   echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
   source ~/.bashrc
   ```

### Q: –ü–æ—á–µ–º—É –≤ v0.0.1-alpha —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–ª–æ?
A: –í–æ–∑–º–æ–∂–Ω–æ, –≤ v0.0.1-alpha –±—ã–ª —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π CUDA-–≤–µ—Ä—Å–∏—è llama-cpp-python,
   –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Ç–æ–º –±—ã–ª –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π).

### Q: –ü–æ–º–æ–≥—É—Ç –ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã offload_kqv, type_k, type_v?
A: –ù–ï–¢! –≠—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ llama-cpp-python —Å–æ–±—Ä–∞–Ω —Å CUDA.
   –ë–µ–∑ CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –Ω–∏–∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –ø–æ–º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU.

### Q: –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch CUDA –¥–ª—è llama-cpp-python?
A: –ù–ï–¢! llama-cpp-python - —ç—Ç–æ C++ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å Python –±–∏–Ω–¥–∏–Ω–≥–∞–º–∏.
   –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç CUDA –Ω–∞–ø—Ä—è–º—É—é, –∞ –Ω–µ —á–µ—Ä–µ–∑ PyTorch.

### Q: –ö–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ —Ç–æ—á–Ω–æ –≤ llama-cpp-python?
A: –ó–∞–ø—É—Å—Ç–∏—Ç–µ `python3 diagnose_cuda.py` - —Å–∫—Ä–∏–ø—Ç —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç –ø—Ä–æ–±–ª–µ–º—É.

### Q: –Ø –æ–±–Ω–æ–≤–∏–ª CUDA –¥–æ 12.x, —á—Ç–æ –¥–µ–ª–∞—Ç—å?
A: –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA 12.1:
   ```bash
   pip uninstall llama-cpp-python -y
   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
   ```

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### CPU-only (–±–µ–∑ CUDA):
- –ó–∞–≥—Ä—É–∑–∫–∞: –í—Å—è –º–æ–¥–µ–ª—å –≤ RAM
- –°–∫–æ—Ä–æ—Å—Ç—å: ~0.1-1 —Ç–æ–∫–µ–Ω/—Å–µ–∫ (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ)
- –ü–∞–º—è—Ç—å: –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM
- GPU: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è

### –° CUDA (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞):
- –ó–∞–≥—Ä—É–∑–∫–∞: –ú–æ–¥–µ–ª—å –≤ VRAM
- –°–∫–æ—Ä–æ—Å—Ç—å: ~10-50 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ (–Ω–æ—Ä–º–∞–ª—å–Ω–æ)
- –ü–∞–º—è—Ç—å: VRAM ~70-80%, RAM –º–∏–Ω–∏–º–∞–ª—å–Ω–æ
- GPU: –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è ~95%

**–†–∞–∑–Ω–∏—Ü–∞:** ~100x —É—Å–∫–æ—Ä–µ–Ω–∏–µ!

---

## üõ†Ô∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏

```bash
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ VRAM –∏ GPU
watch -n 1 nvidia-smi

# –ò–ª–∏ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ
nvtop  # –ù—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: sudo apt install nvtop
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏

–í –ª–æ–≥–∞—Ö –∞–≥–µ–Ω—Ç–∞ (`logs/agent_*.log`) –∏—â–∏—Ç–µ —Å—Ç—Ä–æ–∫–∏:
```
llama_model_load: using CUDA ...
llama_kv_cache_init: CUDA ...
```

–ï—Å–ª–∏ –∏—Ö –Ω–µ—Ç - –º–æ–¥–µ–ª—å –Ω–∞ CPU.

---

## üìù –†–µ–∑—é–º–µ

**–ü—Ä–æ–±–ª–µ–º–∞:** CPU-only –≤–µ—Ä—Å–∏—è llama-cpp-python  
**–†–µ—à–µ–Ω–∏–µ:** –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π  
**–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:** `python3 diagnose_cuda.py`  
**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** –°–º. `requirements-cuda.txt`

**–í–∞–∂–Ω–æ:** –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ agent.py (`offload_kqv`, `type_k`, `type_v`) –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã!
–ü—Ä–æ–±–ª–µ–º–∞ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –≤ —É—Å—Ç–∞–Ω–æ–≤–∫–µ llama-cpp-python.

---

**–í–µ—Ä—Å–∏—è:** v0.0.3-p4-alpha  
**–î–∞—Ç–∞:** 2025-10-18
